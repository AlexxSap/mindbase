#+title: Apache Kafka

=Apache Kafka= — это распределённая платформа для обработки потоковых данных в реальном времени.
Она позволяет публиковать, подписываться на потоки данных (топики), хранить их и обрабатывать в распределённой среде.
=Kafka= используется для создания систем, в которых данные генерируются и обрабатываются постоянно и в больших объёмах. Она предназначена для работы с реальными данными, такими как события, сообщения, логи и т. д.

* Основные компоненты Kafka
Kafka состоит из нескольких ключевых элементов:
- =Producer= (Производитель): приложение или сервис, который публикует данные в Kafka. Производитель записывает события в топики Kafka.
- =Consumer= (Потребитель): приложение или сервис, который подписывается на топики и обрабатывает данные. Потребители могут быть распределены и читать данные параллельно.
- =Broker= (Брокер): сервер, который хранит и управляет данными в Kafka. Брокеры принимают данные от производителей и предоставляют их потребителям. Kafka может работать с множеством брокеров, что делает её распределённой и масштабируемой.
- =Topic= (Топик): логический канал или категория, в которую отправляются данные. Производители публикуют сообщения в топики, а потребители читают сообщения из этих топиков.
- =Partition= (Раздел): топики делятся на разделы для более эффективного хранения и обработки. Каждый раздел — это лог, который хранит сообщения, и Kafka гарантирует, что сообщения в разделе будут обрабатываться в том порядке, в котором они были записаны.
- =ZooKeeper=: это компонент, который Kafka использует для координации и управления метаданными. В новых версиях Kafka можно работать без ZooKeeper, но в традиционных версиях он был необходим для координации между брокерами.

* Как работает Kafka?
Kafka организует обмен сообщениями через топики и разделы, что позволяет обрабатывать большие потоки данных с минимальной задержкой.
Вот как это работает:
1. =Производитель= публикует сообщение в топик Kafka. Сообщения в Kafka являются необработанными данными и могут быть любым типом (например, JSON, Avro, или просто текст).
2. =Сообщение= записывается в определённый раздел топика. Каждый раздел хранится в отдельном файле на диске и является независимым от других разделов.
3. =Потребитель= читает сообщения из топика. Потребитель может подписываться на один или несколько топиков и обрабатывать их в реальном времени. Kafka гарантирует, что сообщения будут обрабатываться по порядку, в котором они были записаны, в рамках одного раздела.
4. =Kafka= обеспечивает репликацию данных, что означает, что каждый раздел может быть скопирован на несколько брокеров для повышения отказоустойчивости.
5. Если =потребитель= обработал сообщение, оно считается "потреблённым", но оно остаётся в журнале Kafka до тех пор, пока не будет удалено по истечении настроенного времени хранения.

* Преимущества Kafka
- Масштабируемость: Kafka может обрабатывать огромные объёмы данных с минимальной задержкой и легко масштабируется.
- Высокая доступность: Репликация данных между брокерами позволяет обеспечить отказоустойчивость.
- Производительность: Kafka может обрабатывать миллионы сообщений в секунду с низкой задержкой.
- Гибкость: Она поддерживает различные типы сообщений и легко интегрируется с другими системами.

* Недостатки Kafka
- Сложность в настройке и обслуживании: Для работы с Kafka необходимо иметь опыт в настройке распределённых систем.
- Проблемы с пропускной способностью в случае высокого объёма данных: В некоторых случаях Kafka может столкнуться с проблемами при работе с очень большими объёмами данных.
- Задержка: Хотя Kafka эффективен в плане пропускной способности, в некоторых случаях он может иметь небольшую задержку при доставке сообщений.

* Пример отправки сообщений
#+begin_src go
package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/segmentio/kafka-go"
)

const (
	topic = "my-learning-topic"
	addr  = "localhost:9093"
)

func main() {
	writer := kafka.NewWriter(kafka.WriterConfig{
		Brokers: []string{addr},
		Topic:   topic,
	})
	defer writer.Close()

	msgs := []string{
		"Всем привет",
		"Всем пока",
	}

	sendMessages(writer, msgs)
	fmt.Println("DONE")
}

func sendMessages(writer *kafka.Writer, msgs []string) {
	for i, msgText := range msgs {
		msg := kafka.Message{
			Key:   []byte(fmt.Sprintf("message №%d", i+1)),
			Value: []byte(msgText),
			Time:  time.Now(),
		}

		err := writer.WriteMessages(context.Background(), msg)
		if err != nil {
			log.Printf("sending error %v\n", err)
		} else {
			log.Printf("message send '%v'\n", msgText)
		}

		time.Sleep(2 * time.Second)

	}
}
#+end_src
* Пример получения сообщений
#+begin_src go
package main

import (
	"context"
	"fmt"
	"log"

	"github.com/segmentio/kafka-go"
)

const (
	topic   = "my-learning-topic"
	addr    = "localhost:9093"
	groupID = "my-learning-go-group"
)

func main() {
	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers: []string{addr},
		Topic:   topic,
		GroupID: groupID,
	})
	defer reader.Close()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	readMessages(ctx, reader)
}

func readMessages(ctx context.Context, reader *kafka.Reader) {
	for {
		msg, err := reader.ReadMessage(ctx)
		if err != nil {
			log.Printf("error in receive message %v", err)
		}

		fmt.Printf("Сообщение в топике %v, партиция %v, offset %v: \n\t%s %s\n\n",
			msg.Topic, msg.Partition, msg.Offset, string(msg.Key), string(msg.Value))
	}
}
#+end_src
