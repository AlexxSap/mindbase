#+title: Apache Kafka

=Apache Kafka= — это распределённая платформа для обработки потоковых данных в реальном времени.
Она позволяет публиковать, подписываться на потоки данных (топики), хранить их и обрабатывать в распределённой среде.
=Kafka= используется для создания систем, в которых данные генерируются и обрабатываются постоянно и в больших объёмах. Она предназначена для работы с реальными данными, такими как события, сообщения, логи и т. д.

* Основные компоненты Kafka
Kafka состоит из нескольких ключевых элементов:
- =Producer= (Производитель): приложение или сервис, который публикует данные в Kafka. Производитель записывает события в топики Kafka.
- =Consumer= (Потребитель): приложение или сервис, который подписывается на топики и обрабатывает данные. Потребители могут быть распределены и читать данные параллельно.
- =Broker= (Брокер): сервер, который хранит и управляет данными в Kafka. Брокеры принимают данные от производителей и предоставляют их потребителям. Kafka может работать с множеством брокеров, что делает её распределённой и масштабируемой.
- =Topic= (Топик): логический канал или категория, в которую отправляются данные. Производители публикуют сообщения в топики, а потребители читают сообщения из этих топиков.
- =Partition= (Раздел): топики делятся на разделы для более эффективного хранения и обработки. Каждый раздел — это лог, который хранит сообщения, и Kafka гарантирует, что сообщения в разделе будут обрабатываться в том порядке, в котором они были записаны.
- =ZooKeeper=: это компонент, который Kafka использует для координации и управления метаданными. В новых версиях Kafka можно работать без ZooKeeper, но в традиционных версиях он был необходим для координации между брокерами.

* Как работает Kafka?
Kafka организует обмен сообщениями через топики и разделы, что позволяет обрабатывать большие потоки данных с минимальной задержкой.
Вот как это работает:
1. =Производитель= публикует сообщение в топик Kafka. Сообщения в Kafka являются необработанными данными и могут быть любым типом (например, JSON, Avro, или просто текст).
2. =Сообщение= записывается в определённый раздел топика. Каждый раздел хранится в отдельном файле на диске и является независимым от других разделов.
3. =Потребитель= читает сообщения из топика. Потребитель может подписываться на один или несколько топиков и обрабатывать их в реальном времени. Kafka гарантирует, что сообщения будут обрабатываться по порядку, в котором они были записаны, в рамках одного раздела.
4. =Kafka= обеспечивает репликацию данных, что означает, что каждый раздел может быть скопирован на несколько брокеров для повышения отказоустойчивости.
5. Если =потребитель= обработал сообщение, оно считается "потреблённым", но оно остаётся в журнале Kafka до тех пор, пока не будет удалено по истечении настроенного времени хранения.

* Преимущества Kafka
- Масштабируемость: Kafka может обрабатывать огромные объёмы данных с минимальной задержкой и легко масштабируется.
- Высокая доступность: Репликация данных между брокерами позволяет обеспечить отказоустойчивость.
- Производительность: Kafka может обрабатывать миллионы сообщений в секунду с низкой задержкой.
- Гибкость: Она поддерживает различные типы сообщений и легко интегрируется с другими системами.

* Недостатки Kafka
- Сложность в настройке и обслуживании: Для работы с Kafka необходимо иметь опыт в настройке распределённых систем.
- Проблемы с пропускной способностью в случае высокого объёма данных: В некоторых случаях Kafka может столкнуться с проблемами при работе с очень большими объёмами данных.
- Задержка: Хотя Kafka эффективен в плане пропускной способности, в некоторых случаях он может иметь небольшую задержку при доставке сообщений.

* Пример отправки сообщений с помощью segmentio/kafka-go
#+begin_src go
package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/segmentio/kafka-go"
)

const (
	topic = "my-learning-topic"
	addr  = "localhost:9093"
)

func main() {
	writer := kafka.NewWriter(kafka.WriterConfig{
		Brokers: []string{addr},
		Topic:   topic,
	})
	defer writer.Close()

	msgs := []string{
		"Всем привет",
		"Всем пока",
	}

	sendMessages(writer, msgs)
	fmt.Println("DONE")
}

func sendMessages(writer *kafka.Writer, msgs []string) {
	for i, msgText := range msgs {
		msg := kafka.Message{
			Key:   []byte(fmt.Sprintf("message №%d", i+1)),
			Value: []byte(msgText),
			Time:  time.Now(),
		}

		err := writer.WriteMessages(context.Background(), msg)
		if err != nil {
			log.Printf("sending error %v\n", err)
		} else {
			log.Printf("message send '%v'\n", msgText)
		}

		time.Sleep(2 * time.Second)

	}
}
#+end_src
* Пример получения сообщений с помощью segmentio/kafka-go
#+begin_src go
package main

import (
	"context"
	"fmt"
	"log"

	"github.com/segmentio/kafka-go"
)

const (
	topic   = "my-learning-topic"
	addr    = "localhost:9093"
	groupID = "my-learning-go-group"
)

func main() {
	reader := kafka.NewReader(kafka.ReaderConfig{
		Brokers: []string{addr},
		Topic:   topic,
		GroupID: groupID,
	})
	defer reader.Close()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	readMessages(ctx, reader)
}

func readMessages(ctx context.Context, reader *kafka.Reader) {
	for {
		msg, err := reader.ReadMessage(ctx)
		if err != nil {
			log.Printf("error in receive message %v", err)
		}

		fmt.Printf("Сообщение в топике %v, партиция %v, offset %v: \n\t%s %s\n\n",
			msg.Topic, msg.Partition, msg.Offset, string(msg.Key), string(msg.Value))
	}
}
#+end_src

* Гарантии доставки в Kafka
Apache Kafka предоставляет три основные гарантии доставки сообщений, которые влияют на надёжность и производительность системы:
- =At most once=
  Это означает, что сообщение будет доставлено не более одного раза. При этом возможна потеря сообщений, так как продюсер может не успеть отправить их на брокер, но повторная отправка не будет происходить.
- =At least once=
  Гарантируется, что сообщение будет доставлено как минимум один раз. Это означает, что если продюсер не получит подтверждения о доставке, то он повторно отправит сообщение. Это стандартное поведение Kafka.
  По умолчанию Kafka использует гарантию At least once для доставки сообщений. Это означает, что если продюсер не получает подтверждения от брокера, он повторно отправит сообщение. В этом случае возможны дубли, но сообщение будет доставлено.
- =Exactly once=
  Это гарантирует, что сообщение будет доставлено ровно один раз. Для этого включаются дополнительные механизмы, такие как идемпотентность и транзакции, чтобы избежать дублирования сообщений, как на стороне продюсера, так и на стороне консьюмера.

#+begin_src go
writer := kafka.NewWriter(kafka.WriterConfig{
		Brokers:      []string{"localhost:9092"},
		Topic:        "my-topic",
		RequiredAcks: -1,               // Подтверждение от всех реплик
		MaxAttempts:  10,               //кол-во попыток доставки(по умолчанию всегда 10)
		BatchSize:    100,              // Ограничение на количество сообщений(по дефолту 100)
		WriteTimeout: 10 * time.Second, //время ожидания для записи(по умолчанию 10сек)
		Balancer:     &kafka.RoundRobin{}, //балансировщик.
	})
#+end_src

Как эти параметры влияют на доставку сообщений:
- RequiredAcks: 0 — минимальная гарантия, сообщения могут быть потеряны(at most once).
- RequiredAcks: 1 — сообщения гарантированно доставляются хотя бы на одного лидера, но могут быть потеряны при сбоях в сети или на брокере((At least once).
- RequiredAcks: -1 — самые надежные гарантии, сообщения подтверждаются всеми репликами, что минимизирует риск потери сообщений, но с увеличением времени ожидания. Значение -1 стоит по умолчанию(At least once, но с дополнительными подтверждениями).

Балансировщик используется для распределения сообщений по партициям.
- RoundRobin - равномерное распределение по партициям.
- LeastBytes - это реализация балансировщика, который направляет сообщения в партиции, получивший наименьшее количество данных.
- Hash - балансировщик по умолчанию для сообщений с ключам.
