#+title: grpc

Важная статья
https://www.cyberforum.ru/blogs/2396861/10270.html?ysclid=mgxcl82n25785829384

=gRPC= — это современный фреймворк для удаленного вызова процедур (RPC), разработанный Google.
Он основан на протоколе HTTP/2 и использует формат сериализации Protocol Buffers (protobuf), что делает его эффективным и быстрым. gRPC отлично подходит для построения микросервисной архитектуры, позволяя сервисам взаимодействовать друг с другом через строго типизированные API.

=gRPC= решает задачу эффективного взаимодействия между сервисами.
В отличие от REST API, который использует текстовый формат JSON и HTTP/1.1, gRPC работает поверх HTTP/2 и использует бинарный формат Protocol Buffers.

Это дает несколько преимуществ:
- Высокая производительность — бинарная сериализация быстрее и компактнее JSON.
- Поддержка потоковой передачи данных — gRPC позволяет реализовывать стриминговые вызовы.
- Языковая независимость — клиенты и серверы могут быть написаны на разных языках.
- Автоматическая генерация кода — API описываются в файлах .proto, а на их основе создаются серверные и клиентские обертки.

Если вам важна производительность, строгая типизация и поддержка потоков — gRPC будет лучшим выбором. Если же нужно простое взаимодействие между сервисами без сложной настройки, REST может быть более удобным.

=Protocol Buffers= (или просто =Protobuf=) — бинарный формат сериализации, который значительно компактнее и быстрее в обработке, чем XML или JSON. Важная особеность Protobuf — =строгая типизация=.
Структуры данных определяются в специальных =.proto= файлах, на основе которых генерируются классы для различных языков программирования. Это гарантирует, что и клиент, и сервер "говорят на одном языке", минимизируя риск ошибок при передаче данных.

* Способы взаимодействия
Традиционная модель "запрос-ответ" расширяется до четырёх различных типов взаимодействия:
1. =Унарные вызовы= (Unary RPCs) — классический паттерн "клиент отправляет один запрос, сервер возвращает один ответ".
2. =Серверные потоковые вызовы= (Server streaming RPCs) — клиент отправляет один запрос, а сервер может вернуть поток ответов.
   Определение:
#+begin_src
...
rpc searchOrders(google.protobuf.StringValue) returns (stream Order);
...
#+end_src
  Реализация:
#+begin_src go
func (s *server) SearchOrders(searchQuery *wrappers.StringValue,
stream pb.OrderManagement_SearchOrdersServer) error {
...
	err := stream.Send(&order)
...

}
#+end_src
Удаленный вызов методов на клиентской стороне сильно напоминает простой RPC. Но в данном случае необходимо обрабатывать множественные ответы, поскольку сервер записывает в поток цепочку ответов. Таким образом, в реализации gRPC-клиента на Go мы последовательно извлекаем сообщения из клиентского потока, используя метод Recv(), и делаем это до тех пор, пока данный поток не закончится.
#+begin_src go
searchStream, _ := c.SearchOrders(ctx, &wrapper.StringValue{Value: "Google"})
for {
	searchOrder, err := searchStream.Recv()
	...
	if err == io.EOF {
		break
	}
}
#+end_src

3. =Клиентские потоковые вызовы= (Client streaming RPCs) — клиент отправляет поток запросов, а сервер возвращает один ответ.
   Определение
#+begin_src
service OrderManagement {
...
rpc updateOrders(stream Order) returns (google.protobuf.StringValue);
...
}
#+end_src

В этом случае клиент посылает множество запросов, а сервер их принимает в цикле

4. =Двунаправленные потоковые вызовы= (Bidirectional streaming RPCs) — клиент и сервер могут обмениваться потоками сообщений в любом порядке.

* Жизненный цикл запроса
Жизненный цикл gRPC-запроса представляет собой последовательность событий:
1. Клиентская заглушка (stub) упаковывает параметры метода в Protobuf-сообщение.
2. Клиентская библиотека gRPC сериализует это сообщение в бинарный формат.
3. Запрос отправляется серверу по HTTP/2.
4. На стороне сервера gRPC десериализует полученное сообщение.
5. Серверная заглушка (skeleton) вызывает соответствующий пользовательский метод с полученными параметрами.
6. Результат метода упаковывается, сериализуется и отправляется обратно клиенту.
7. Клиентская библиотека десериализует ответ и передаёт его вызывающему коду.

* Балансировка
В отличие от REST, балансировка нагрузки в gRPC-системах имеет свои нюансы из-за использования HTTP/2. Традиционные L4/L7 балансировщики, разработанные для HTTP/1.x, часто не могут эффективно распределять нагрузку между несколькими gRPC-серверами, поскольку HTTP/2 использует долгоживущие соединения и мультиплексирование.
gRPC предлагает два основных подхода к балансировке нагрузки: прокси-балансировка и клиентская балансировка. Прокси-балансировка опирается на выделенный балансировщик (например, Envoy или NGINX с соответствующими модулями), который понимает специфику HTTP/2 и может правильно распределять запросы. Клиентская балансировка, напротив, перемещает логику балансировки непосредственно в клиентские библиотеки.
Клиентская балансировка имеет интересные преимущества — она устраняет дополнительный прыжок в сети и потенциальное узкое место в виде централизованого балансировщика. Однако требует механизма обнаружения сервисов (service discovery), чтобы клиент знал, какие именно экземпляры сервисов доступны в данный момент.
В продакшн-средах часто используется гибридный подход: клиент обращается к именованному сервису через DNS, а за этим именем скрывается балансировщик, который распределяет запросы между фактическими экземплярами. Этот подход хорошо работает с Kubernetes и другими современными оркестраторами.

* Структура и особенности Protocol Buffers
=Protocol Buffers= (или =Protobuf=) — один из краеугольных камней экосистемы gRPC.
Сердцем любого Protobuf-решения являются .proto файлы — своего рода нейтральное к языкам программирования описание структур данных и сервисов. Эти файлы становяться контрактом между разными частями распределённой системы, гарантируя, что все участники "разговора" понимают друг друга.
#+begin_src go
syntax = "proto3"; // Указываем версию синтаксиса

package users.management; // Определяем пакет для предотвращения конфликтов имен

// Импорт определений из других .proto файлов
import "common/types.proto";

// Определяем сервис - набор методов, которые можно вызывать удаленно
service UserManagement {
  // Унарный метод: один запрос, один ответ
  rpc GetUser(GetUserRequest) returns (User);

  // Серверный потоковый метод: один запрос, поток ответов
  rpc ListUsers(ListUsersRequest) returns (stream User);

  // Клиентский потоковый метод: поток запросов, один ответ
  rpc BatchCreateUsers(stream CreateUserRequest) returns (BatchResponse);

  // Двунаправленный потоковый метод: оба участника отправляют потоки сообщений
  rpc ChatWithSupport(stream ChatMessage) returns (stream ChatMessage);
}

// Определение сообщения - структуры данных
message User {
  string id = 1; // Каждое поле имеет уникальный номер (тэг)
  string name = 2;
  string email = 3;
  UserStatus status = 4; // Использование перечисления
  repeated string roles = 5; // Массив строк
  map<string, string> metadata = 6; // Ассоциативный массив

  // Вложенный тип, видимый только внутри User
  message Address {
    string street = 1;
    string city = 2;
    string postal_code = 3;
    string country = 4;
  }

  repeated Address addresses = 7; // Массив вложенных объектов

  oneof contact { // Только одно из полей может быть установлено
    string phone_number = 8;
    string alternative_email = 9;
  }

  common.Timestamp created_at = 10; // Импортированный тип
}

// Перечисление - набор именованных констант
enum UserStatus {
  UNKNOWN = 0; // Первое значение должно быть 0
  ACTIVE = 1;
  SUSPENDED = 2;
  DELETED = 3;
}

// Другие сообщения для запросов и ответов
message GetUserRequest {
  string user_id = 1;
}

message ListUsersRequest {
  int32 page_size = 1;
  string page_token = 2;
  string filter = 3;
}

message CreateUserRequest {
  User user = 1;
}

message BatchResponse {
  int32 success_count = 1;
  int32 failure_count = 2;
  repeated string error_messages = 3;
}

message ChatMessage {
  string sender = 1;
  string content = 2;
  common.Timestamp sent_at = 3;
}
#+end_src

Кроме базовых типов, Protobuf поддерживает комплексные типы данных и специальные конструкции:
1. =Вложенные типы= — можно определять сообщения и перечисления внутри других сообщений, что помогает организовывать сложные схемы данных.
2. =Repeated= поля — аналог массивов или списков, позволяющие хранить несколько значений одного типа.
3. =Oneof= — специальная конструкция для моделирования взаимоисключающих полей, когда только одно из нескольких полей может быть установлено.
4. =Map= — ассоциативные массивы, появившиеся в Proto3.
5. =Расширения= (Extensions) — в Proto2 позволяют добавлять поля к существующим сообщениям без изменения их определения (в Proto3 заменены типом Any).

Помимо типов данных и генерации кода, Protocol Buffers также предоставляют богатые возможности для валидации и документирования схемы данных.
С помощью комментариев и специальных аннотации в .proto файлах, можно создавать самодокументируемые контракты API:
#+begin_src go
// Пользователь системы
message User {
  // Уникальный идентификатор пользователя
  // Должен соответствовать формату UUID v4
  string id = 1 [(validate.rules).string.pattern = "^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$"];

  // Полное имя пользователя
  string name = 2 [(validate.rules).string.min_len = 2, (validate.rules).string.max_len = 100];

  // Email пользователя для связи
  string email = 3 [(validate.rules).string.email = true];

  // ...
}
#+end_src

Такой подход к документации и валидации, встроенный прямо в схему данных, значительно упрощает поддержку и развитие микросервисной архитектуры, особенно когда над ней работает несколько команд.

При изменении схемы .proto файлов нужно быть уверенным, что эти изменения не нарушат работу существующих клиентов. Хотя Protobuf обеспечивает определёную степень обратной совместимости, есть операции, которые могут её нарушить:
- Удаление полей или изменение их типов.
- Изменение тегов (номеров) полей.
- Переименование полей (хотя сам Protobuf этого не "видит", но сгенерированный код изменится).

Поэтому в продакшн-системах мы обычно следуем следущему подходу:
1. Никогда не удаляем поля — вместо этого помечаем их как устаревшие (deprecated).
2. Никогда не меняем теги полей — даже если поле переименовывается, его тег должен остаться прежним.
3. Контролируем обратную совместимость автоматически с помощью инструментов типа protolock.

* Установка gRPC в Go
Перед началом работы необходимо установить пакет gRPC и компилятор Protocol Buffers:
#+begin_src
go install google.golang.org/protobuf/cmd/protoc@latest
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest
#+end_src

Эти инструменты нужны для генерации Go-кода из .proto файлов. Теперь добавим зависимости в проект:
#+begin_src
go get google.golang.org/grpc
go get google.golang.org/protobuf
#+end_src

* Установка gRPC в С++
#+begin_src
# Установка базовых инструментов
sudo apt-get update
sudo apt-get install -y build-essential cmake autoconf libtool pkg-config

# Клонирование и установка gRPC вместе с Protobuf
git clone --recurse-submodules -b v1.76.0 https://github.com/grpc/grpc
cd grpc
mkdir -p cmake/build
cd cmake/build
cmake -DgRPC_INSTALL=ON -DgRPC_BUILD_TESTS=OFF -DCMAKE_INSTALL_PREFIX=$HOME/.local ../..
make -j$(nproc)
make install
#+end_src

* Определение gRPC-сервиса
В gRPC API описывается с помощью файла =.proto=. Давайте создадим сервис для управления пользователями:
#+begin_src go
syntax = "proto3";

package main;

option go_package = "./pb";

service Greeter {
  rpc SayHello (HelloRequest) returns (HelloReply);
}

message HelloRequest {
  string name = 1;
}

message HelloReply {
  string message = 1;
}
#+end_src

Теперь сгенерируем код для Go:
#+begin_src
protoc --go_out=. --go-grpc_out=. hello.proto
#+end_src

* Реализация сервера
#+begin_src go
package main

import (
	"context"
	"log"
	"net"

	"google.golang.org/grpc"
	pb "grpcex/pb" // Импортируем сгенерированный код
)

type server struct {
	pb.UnimplementedGreeterServer
}

func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {
	return &pb.HelloReply{Message: "Hello " + in.Name}, nil
}

func main() {
	lis, err := net.Listen("tcp", ":50051")
	if err != nil {
		log.Fatalf("Failed to listen: %v", err)
	}

	s := grpc.NewServer()
	pb.RegisterGreeterServer(s, &server{})

	log.Println("Server listening on :50051")
	if err := s.Serve(lis); err != nil {
		log.Fatalf("Failed to serve: %v", err)
	}
}
#+end_src

* Реализация клиента
#+begin_src go
package main

import (
	"context"
	"log"
	"time"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure" // Используем insecure для упрощения
	pb "grpcex/pb" // Импортируем сгенерированный код
)

func main() {
	conn, err := grpc.Dial("localhost:50051", grpc.WithTransportCredentials(insecure.NewCredentials()))
	if err != nil {
		log.Fatalf("Failed to connect: %v", err)
	}
	defer conn.Close()

	client := pb.NewGreeterClient(conn)

	resp, err := client.SayHello(context.Background(), &pb.HelloRequest{Name: "World"})
	if err != nil {
		log.Fatalf("Failed to call SayHello: %v", err)
	}

	log.Printf("Response: %s", resp.Message)
}
#+end_src

* Перехватчики
Иногда перед вызовом удаленной функции на клиентской или серверной стороне или после него нужно выполнить некие рутинные операции. На этот случай gRPC позволяет перехватывать вызов для выполнения таких задач, как ведение журнала, аутентификация, сбор метрик и пр., используя механизм расширения под названием «перехватчик» (interceptor).
В унарном RPC можно использовать =унарные= перехватчики, а в потоковом — =потоковые=.
** 1. Унарный серверный перехватчик
   Это тип серверных унарных перехватчиков со следующей сигнатурой:
#+begin_src go
func(ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler) (resp interface{}, err error)
#+end_src

Пример:
#+begin_src go
// унарный перехватчик на стороне сервера
func orderUnaryServerInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler)
(interface{}, error) {
	// логика перед вызовом
	// получает информацию о текущем RPC-вызове путем
	// анализа переданных аргументов
	log.Println("======= [Server Interceptor] ", info.FullMethod)
	// вызываем обработчик, чтобы завершить
	// нормальное выполнение унарного RPC-вызова
	m, err := handler(ctx, req)

	// логика после вызова
	log.Printf(" Post Proc Message : %s", m)
	return m, err
}

// ...
func main() {
...
	// регистрируем перехватчик на стороне сервера
	s := grpc.NewServer(grpc.UnaryInterceptor(orderUnaryServerInterceptor))
...
#+end_src

** 2. Потоковый серверный перехватчик
Функция перехвата имеет следующую сигнатуру:
#+begin_src go
func(srv interface{}, ss ServerStream, info *StreamServerInfo, handler StreamHandler) error
#+end_src

Пример:
#+begin_src go
// Потоковый перехватчик на стороне сервера
// wrappedStream — обертка вокруг встроенного интерфейса
// grpc.ServerStream, которая перехватывает вызовы методов
// RecvMsg и SendMsg
type wrappedStream struct {
	grpc.ServerStream
}

// Реализация функции RecvMsg, принадлежащей обертке; обрабатывает сообщения, принимаемые с помощью потокового RPC.
func (w *wrappedStream) RecvMsg(m interface{}) error {
	log.Printf("====== [Server Stream Interceptor Wrapper] " + "Receive a message (Type: %T) at %s",
		m,
		time.Now().Format(time.RFC3339))

	return w.ServerStream.RecvMsg(m)
}

// Реализация функции SendMsg, принадлежащей обертке; обрабатывает сообщения, отправляемые с помощью потокового RPC.
func (w *wrappedStream) SendMsg(m interface{}) error {
	log.Printf("====== [Server Stream Interceptor Wrapper] " + "Send a message (Type: %T) at %v",
		m,
		time.Now().Format(time.RFC3339))

	return w.ServerStream.SendMsg(m)
}

func newWrappedStream(s grpc.ServerStream) grpc.ServerStream {
	return &wrappedStream{s}
}

// Реализация потокового перехватчика
func orderServerStreamInterceptor(srv interface{},
	ss grpc.ServerStream,
	info *grpc.StreamServerInfo,
	handler grpc.StreamHandler) error {
	log.Println("====== [Server Stream Interceptor] ",
		info.FullMethod)
	err := handler(srv, newWrappedStream(ss))

	if err != nil {
		log.Printf("RPC failed with error %v", err)
	}
	return err
}
...
// регистрация перехватчика
s := grpc.NewServer(grpc.StreamInterceptor(orderServerStreamInterceptor))
...
#+end_src

** 3. Унарный клиентский перехватчик
Он представляет собой функцию типа =UnaryClientInterceptor= со следующей сигнатурой:
#+begin_src go
func(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error
#+end_src

Пример:
#+begin_src go
func orderUnaryClientInterceptor(ctx context.Context,
	method string,
	req,
	reply interface{},
	cc *grpc.ClientConn,
	invoker grpc.UnaryInvoker,
	opts ...grpc.CallOption) error {

	// этап предобработки
	log.Println("Method : " + method)
	// вызов удаленного метода
	err := invoker(ctx, method, req, reply, cc, opts...)
	// этап постобработки
	log.Println(reply)

	return err
}

func main() {
	// установление соединения с сервером
	conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithUnaryInterceptor(orderUnaryClientInterceptor))
...
#+end_src

** 4. Потоковый клиентский перехватчик
Он представляет собой функцию типа StreamClientInterceptor со следующей сигнатурой:
#+begin_src go
func(ctx context.Context,
	desc *StreamDesc,
	cc *ClientConn,
	method string,
	streamer Streamer,
	opts ...CallOption) (ClientStream, error)
#+end_src

Пример:
#+begin_src go
func clientStreamInterceptor(
	ctx context.Context,
	desc *grpc.StreamDesc,
	cc *grpc.ClientConn,
	method string,
	streamer grpc.Streamer,
	opts ...grpc.CallOption) (grpc.ClientStream, error) {

	log.Println("======= [Client Interceptor] ", method)
	s, err := streamer(ctx, desc, cc, method, opts...)

	if err != nil {
		return nil, err
	}
	return newWrappedStream(s), nil
}

type wrappedStream struct {
	grpc.ClientStream
}

func (w *wrappedStream) RecvMsg(m interface{}) error {
	log.Printf("====== [Client Stream Interceptor] " + "Receive a message (Type: %T) at %v",
		m,
		time.Now().Format(time.RFC3339))
	return w.ClientStream.RecvMsg(m)
}

func (w *wrappedStream) SendMsg(m interface{}) error {
	log.Printf("====== [Client Stream Interceptor] " + "Send a message (Type: %T) at %v",
		m, time.Now().Format(time.RFC3339))
	return w.ClientStream.SendMsg(m)
}

func newWrappedStream(s grpc.ClientStream) grpc.ClientStream {
	return &wrappedStream{s}
}
...
func main() {
// установление соединения с сервером
conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithStreamInterceptor(clientStreamInterceptor))
...
#+end_src

* Метаданные
Процесс создания метаданных в gRPC-приложении довольно прост и понятен. В следующем фрагменте кода на языке Go показано сразу два способа.
#+begin_src go
// создание метаданных: первый вариант
md := metadata.New(map[string]string{"key1": "val1", "key2": "val2"})
// создание метаданных: второй вариант
md := metadata.Pairs(
	"key1", "val1",
	"key1", "val1-2", // у "key1" будет значение []string{"val1", "val1-2"}
	"key2", "val2",
)
#+end_src
Чтение метаданных на стороне клиента и сервера можно выполнять с помощью входящего контекста, используя вызов metadata.FromIncomingContext(ctx), который в языке Go возвращает хеш-таблицу с метаданными:
#+begin_src go
func (s *server) AddOrder(ctx context.Context, orderReq *pb.Order) (*wrappers.StringValue, error) {
	md, metadataAvailable := metadata.FromIncomingContext(ctx)
	// читаем нужные нам метаданные из хеш-таблицы 'md'
#+end_src

** Отправка и получение метаданных на стороне клиента
Чтобы отправить метаданные gRPC-сервису, их нужно создать и указать в контексте удаленного вызова.
Можно создать метаданные вместе с новым контекстом, используя функцию =NewOutgoingContext=, или просто добавить их в существующий контекст, вызвав =AppendToOutgoingContext=. Обратите внимание: в первом варианте любые имеющиеся в контексте метаданные будут заменены.
#+begin_src go
md := metadata.Pairs(
	"timestamp", time.Now().Format(time.StampNano),
	"kn", "vn",
)
mdCtx := metadata.NewOutgoingContext(context.Background(), md)
ctxA := metadata.AppendToOutgoingContext(mdCtx, "k1", "v1", "k1", "v2", "k2", "v3")

// делаем унарный удаленный вызов
response, err := client.SomeRPC(ctxA, someRequest)
// или делаем потоковый удаленный вызов
stream, err := client.SomeStreamingRPC(ctxA)
#+end_src

Чтение метаданных на стороне клиента
#+begin_src go
// Переменная для хранения заголовка и заключительного блока, возвращенных удаленным вызовом.
var header, trailer metadata.MD
// ***** унарный RPC *****
// Передаем ссылку на заголовок и заключительный блок, чтобы сохранить значения для унарного RPC.
r, err := client.SomeRPC(
	ctx,
	someRequest,
	grpc.Header(&header),
	grpc.Trailer(&trailer),
)
// обрабатываем здесь хеш-таблицу с заголовками и заключительными блоками

// ***** потоковый RPC *****
stream, err := client.SomeStreamingRPC(ctx)
// извлекаем заголовок
header, err := stream.Header()
// извлекаем заключительный блок
trailer := stream.Trailer()
// обрабатываем хеш-таблицу с заголовками и заключительными блоками
#+end_src

** Отправка и получение метаданных на стороне сервера
Получение метаданных на стороне сервера — довольно простой и понятный процесс. В языке Go для этого достаточно выполнить =metadata.FromIncomingContext(ctx)= внутри реализации вашего удаленного метода
#+begin_src go
func (s *server) SomeRPC(ctx context.Context, in *pb.someRequest) (*pb.someResponse, error) {
	md, ok := metadata.FromIncomingContext(ctx)
// некие действия с метаданными
}
func (s *server) SomeStreamingRPC(stream pb.Service_SomeStreamingRPCServer) error {
	md, ok := metadata.FromIncomingContext(stream.Context())
// некие действия с метаданными
}
#+end_src

Чтобы отправить метаданные на серверной стороне, задействуйте заголовок или заключительный блок.
#+begin_src go
func (s *server) SomeRPC(ctx context.Context, in *pb.someRequest) (*pb.someResponse, error) {
	// создаем и отправляем заголовок
	header := metadata.Pairs("header-key", "val")
	grpc.SendHeader(ctx, header)
	// создаем и отправляем заключительный блок
	trailer := metadata.Pairs("trailer-key", "val")
	grpc.SetTrailer(ctx, trailer)
}
func (s *server) SomeStreamingRPC(stream pb.Service_SomeStreamingRPCServer) error {
	// создаем и отправляем заголовок
	header := metadata.Pairs("header-key", "val")
	stream.SendHeader(header)
	// создаем и отправляем заключительный блок
	trailer := metadata.Pairs("trailer-key", "val") stream.SetTrailer(trailer)
}
#+end_src
И в унарном, и в потоковом стиле метаданные можно отправлять с помощью метода =grpc.SendHeader=. Вы также можете сделать их частью заключительного блока в текущем контексте, используя метод =grpc.SetTrailer= или =SetTrailer= соответствующего потока.

* Аутентификация gRPC-канала с помощью TLS
Протокол защиты транспортного уровня (Transport Level Security, TLS) предназначен для шифрования и обеспечения целостности данных, передающихся между приложениями. В gRPC он используется для установления безопасного соединения между клиентом и сервером.
Защитить передачу данных между клиентом и сервером можно с помощью однонаправленного подхода, а также двунаправленного (известного как взаимный TLS, или mTLS).

** 1. Однонаправленное защищенное соединение
В однонаправленном соединении проверкой подлинности занимается только клиент. В начале сессии сервер передает клиенту свой открытый сертификат, чтобы тот его проверил. Для этого используется удостоверяющий центр (certificate authority, CA). Убе    дившись в подлинности сертификата, клиент отправляет данные, зашифрованные с помощью закрытого ключа.
Включить TLS можно, сначала создав следующие сертификаты и ключи:
- server.key — закрытый RSA-ключ, с помощью которого подписывается и аутентифицируется открытый ключ;
- server.pem/server.crt — самоподписанные открытые X.509-ключи для публичного распространения.

Включение однонаправленного безопасного соединения на стороне сервера
Это самый простой способ зашифровать взаимодействие клиента и сервера. Сервер должен быть инициализирован с помощью открытого и закрытого ключей.
#+begin_src go
var (
	port = ":50051"
	crtFile = "server.crt"
	keyFile = "server.key"
)

func main() {
	// Считываем и анализируем открытый/закрытый ключи и создаем сертификат, чтобы включить TLS.
	cert, err := tls.LoadX509KeyPair(crtFile,keyFile)
	if err != nil {
		log.Fatalf("failed to load key pair: %s", err)
	}
	// Включаем TLS для всех входящих соединений, используя сертификаты для аутентификации.
	opts := []grpc.ServerOption{
		grpc.Creds(credentials.NewServerTLSFromCert(&cert))
	}
	s := grpc.NewServer(opts...)
	...
#+end_src

Включение однонаправленного безопасного соединения на стороне клиента
Чтобы подключиться к серверу, клиент должен получить его самоподписанный публичный сертификат.
#+begin_src go
var (
	address = "localhost:50051"
	hostname = "localhost"
	crtFile = "server.crt"
)

func main() {
	// Считываем и анализируем публичный сертификат, чтобы включить TLS.
	creds, err := credentials.NewClientTLSFromFile(crtFile, hostname)
	if err != nil {
		log.Fatalf("failed to load credentials: %v", err)
	}
	// Указываем аутентификационные данные для транспортного протокола с помощью DialOption
	opts := []grpc.DialOption{
		grpc.WithTransportCredentials(creds),
	}
	conn, err := grpc.Dial(address, opts...)
#+end_src

** 2. Включение безопасного соединения mTLS
Основное назначение mTLS состоит в том, чтобы сервер мог контролировать клиентские приложения, которые подключаются к нему. В отличие от однонаправленного TLS-соединения сервер принимает запросы от ограниченной группы проверенных клиентов. Обе стороны обмениваются друг с другом публичными сертификатами и проверяют их подлинность.
Соединение устанавливается по следующему принципу:
1. Клиент отправляет серверу запрос, чтобы получить доступ к защищенной информации.
2. Сервер возвращает клиенту сертификат X.509.
3. Клиент проверяет полученный сертификат в соответствующем удостоверяющем центре.
4. В случае успешной проверки клиент передает серверу свой сертификат.
5. Теперь уже сервер проверяет сертификат клиента в удостоверяющем центре.
6. В случае успеха сервер открывает доступ к защищенным данным.

Представим, что у нас уже есть все сертификаты, необходимые для включения mTLS в целях клиент-серверного взаимодействия. В результате корректной процедуры генерации у вас должны получиться следующие ключи и сертификаты:
- server.key — закрытый RSA-ключ сервера;
- server.crt — публичный сертификат сервера;
- client.key — закрытый RSA-ключ клиента;
- client.crt — публичный сертификат клиента;
- ca.crt — публичный сертификат удостоверяющего цент

Включение mTLS на gRPC-сервере
#+begin_src go
var (
	port = ":50051"
	crtFile = "server.crt"
	keyFile = "server.key"
	caFile = "ca.crt"
)

// Создаем пары ключей X.509 непосредственно из ключа и сертификата сервера.
certificate, err := tls.LoadX509KeyPair(crtFile, keyFile)
if err != nil {
	log.Fatalf("failed to load key pair: %s", err)
}
// Генерируем пул сертификатов в удостоверяющем центре.
certPool := x509.NewCertPool()
ca, err := ioutil.ReadFile(caFile)
if err != nil {
	log.Fatalf("could not read ca certificate: %s", err)
}
// Добавляем клиентские сертификаты из удостоверяющего центра в сгенерированный пул.
if ok := certPool.AppendCertsFromPEM(ca); !ok {
	log.Fatalf("failed to append ca certificate")
}

opts := []grpc.ServerOption{
// включаем TLS для всех входящих соединений
grpc.Creds(credentials.NewTLS(&tls.Config {
	ClientAuth: tls.RequireAndVerifyClientCert,
	Certificates: []tls.Certificate{certificate},
	ClientCAs: certPool,
},)),}

s := grpc.NewServer(opts...)
#+end_src

Включение mTLS для gRPC-клиента
#+begin_src go
var (
	address = "localhost:50051"
	hostname = "localhost"
	crtFile = "client.crt"
	keyFile = "client.key"
	caFile = "ca.crt"
)
// Создаем пары ключей X.509 непосредственно из ключа и сертификата сервера.
certificate, err := tls.LoadX509KeyPair(crtFile, keyFile)
if err != nil {
	log.Fatalf("could not load client key pair: %s", err)
}
// Генерируем пул сертификатов в удостоверяющем центре.
certPool := x509.NewCertPool()
ca, err := ioutil.ReadFile(caFile)
if err != nil {
	log.Fatalf("could not read ca certificate: %s", err)
}
// Добавляем клиентские сертификаты из удостоверяющего центра в сгенерированный пул.
if ok := certPool.AppendCertsFromPEM(ca); !ok {
	log.Fatalf("failed to append ca certs")
}
// Указываем транспортные аутентификационные данные в виде параметров соединения.
// Поле ServerName должно быть равно значению Common Name,указанному в сертификате.
opts := []grpc.DialOption{grpc.WithTransportCredentials( credentials.NewTLS(&tls.Config{
	ServerName: hostname, // Примечание: это обязательно!
	Certificates: []tls.Certificate{certificate},
	RootCAs: certPool,
})),
}

conn, err := grpc.Dial(address, opts...)
#+end_src
