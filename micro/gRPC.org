#+title: grpc

Важная статья
https://www.cyberforum.ru/blogs/2396861/10270.html?ysclid=mgxcl82n25785829384

=gRPC= — это современный фреймворк для удаленного вызова процедур (RPC), разработанный Google.
Он основан на протоколе HTTP/2 и использует формат сериализации Protocol Buffers (protobuf), что делает его эффективным и быстрым. gRPC отлично подходит для построения микросервисной архитектуры, позволяя сервисам взаимодействовать друг с другом через строго типизированные API.

=gRPC= решает задачу эффективного взаимодействия между сервисами.
В отличие от REST API, который использует текстовый формат JSON и HTTP/1.1, gRPC работает поверх HTTP/2 и использует бинарный формат Protocol Buffers.

Это дает несколько преимуществ:
- Высокая производительность — бинарная сериализация быстрее и компактнее JSON.
- Поддержка потоковой передачи данных — gRPC позволяет реализовывать стриминговые вызовы.
- Языковая независимость — клиенты и серверы могут быть написаны на разных языках.
- Автоматическая генерация кода — API описываются в файлах .proto, а на их основе создаются серверные и клиентские обертки.

Если вам важна производительность, строгая типизация и поддержка потоков — gRPC будет лучшим выбором. Если же нужно простое взаимодействие между сервисами без сложной настройки, REST может быть более удобным.

=Protocol Buffers= (или просто =Protobuf=) — бинарный формат сериализации, который значительно компактнее и быстрее в обработке, чем XML или JSON. Важная особеность Protobuf — =строгая типизация=.
Структуры данных определяются в специальных =.proto= файлах, на основе которых генерируются классы для различных языков программирования. Это гарантирует, что и клиент, и сервер "говорят на одном языке", минимизируя риск ошибок при передаче данных.

* Способы взаимодействия
Традиционная модель "запрос-ответ" расширяется до четырёх различных типов взаимодействия:
1. =Унарные вызовы= (Unary RPCs) — классический паттерн "клиент отправляет один запрос, сервер возвращает один ответ".
2. =Серверные потоковые вызовы= (Server streaming RPCs) — клиент отправляет один запрос, а сервер может вернуть поток ответов.
   Определение:
#+begin_src
...
rpc searchOrders(google.protobuf.StringValue) returns (stream Order);
...
#+end_src
  Реализация:
#+begin_src go
func (s *server) SearchOrders(searchQuery *wrappers.StringValue,
stream pb.OrderManagement_SearchOrdersServer) error {
...
	err := stream.Send(&order)
...

}
#+end_src
Удаленный вызов методов на клиентской стороне сильно напоминает простой RPC. Но в данном случае необходимо обрабатывать множественные ответы, поскольку сервер записывает в поток цепочку ответов. Таким образом, в реализации gRPC-клиента на Go мы последовательно извлекаем сообщения из клиентского потока, используя метод Recv(), и делаем это до тех пор, пока данный поток не закончится.
#+begin_src go
searchStream, _ := c.SearchOrders(ctx, &wrapper.StringValue{Value: "Google"})
for {
	searchOrder, err := searchStream.Recv()
	...
	if err == io.EOF {
		break
	}
}
#+end_src

3. =Клиентские потоковые вызовы= (Client streaming RPCs) — клиент отправляет поток запросов, а сервер возвращает один ответ.
   Определение
#+begin_src
service OrderManagement {
...
rpc updateOrders(stream Order) returns (google.protobuf.StringValue);
...
}
#+end_src

В этом случае клиент посылает множество запросов, а сервер их принимает в цикле

4. =Двунаправленные потоковые вызовы= (Bidirectional streaming RPCs) — клиент и сервер могут обмениваться потоками сообщений в любом порядке.

* Жизненный цикл запроса
Жизненный цикл gRPC-запроса представляет собой последовательность событий:
1. Клиентская заглушка (stub) упаковывает параметры метода в Protobuf-сообщение.
2. Клиентская библиотека gRPC сериализует это сообщение в бинарный формат.
3. Запрос отправляется серверу по HTTP/2.
4. На стороне сервера gRPC десериализует полученное сообщение.
5. Серверная заглушка (skeleton) вызывает соответствующий пользовательский метод с полученными параметрами.
6. Результат метода упаковывается, сериализуется и отправляется обратно клиенту.
7. Клиентская библиотека десериализует ответ и передаёт его вызывающему коду.

* Балансировка
В отличие от REST, балансировка нагрузки в gRPC-системах имеет свои нюансы из-за использования HTTP/2. Традиционные L4/L7 балансировщики, разработанные для HTTP/1.x, часто не могут эффективно распределять нагрузку между несколькими gRPC-серверами, поскольку HTTP/2 использует долгоживущие соединения и мультиплексирование.
gRPC предлагает два основных подхода к балансировке нагрузки: прокси-балансировка и клиентская балансировка. Прокси-балансировка опирается на выделенный балансировщик (например, Envoy или NGINX с соответствующими модулями), который понимает специфику HTTP/2 и может правильно распределять запросы. Клиентская балансировка, напротив, перемещает логику балансировки непосредственно в клиентские библиотеки.
Клиентская балансировка имеет интересные преимущества — она устраняет дополнительный прыжок в сети и потенциальное узкое место в виде централизованого балансировщика. Однако требует механизма обнаружения сервисов (service discovery), чтобы клиент знал, какие именно экземпляры сервисов доступны в данный момент.
В продакшн-средах часто используется гибридный подход: клиент обращается к именованному сервису через DNS, а за этим именем скрывается балансировщик, который распределяет запросы между фактическими экземплярами. Этот подход хорошо работает с Kubernetes и другими современными оркестраторами.

* Структура и особенности Protocol Buffers
=Protocol Buffers= (или =Protobuf=) — один из краеугольных камней экосистемы gRPC.
Сердцем любого Protobuf-решения являются .proto файлы — своего рода нейтральное к языкам программирования описание структур данных и сервисов. Эти файлы становяться контрактом между разными частями распределённой системы, гарантируя, что все участники "разговора" понимают друг друга.
#+begin_src go
syntax = "proto3"; // Указываем версию синтаксиса

package users.management; // Определяем пакет для предотвращения конфликтов имен

// Импорт определений из других .proto файлов
import "common/types.proto";

// Определяем сервис - набор методов, которые можно вызывать удаленно
service UserManagement {
  // Унарный метод: один запрос, один ответ
  rpc GetUser(GetUserRequest) returns (User);

  // Серверный потоковый метод: один запрос, поток ответов
  rpc ListUsers(ListUsersRequest) returns (stream User);

  // Клиентский потоковый метод: поток запросов, один ответ
  rpc BatchCreateUsers(stream CreateUserRequest) returns (BatchResponse);

  // Двунаправленный потоковый метод: оба участника отправляют потоки сообщений
  rpc ChatWithSupport(stream ChatMessage) returns (stream ChatMessage);
}

// Определение сообщения - структуры данных
message User {
  string id = 1; // Каждое поле имеет уникальный номер (тэг)
  string name = 2;
  string email = 3;
  UserStatus status = 4; // Использование перечисления
  repeated string roles = 5; // Массив строк
  map<string, string> metadata = 6; // Ассоциативный массив

  // Вложенный тип, видимый только внутри User
  message Address {
    string street = 1;
    string city = 2;
    string postal_code = 3;
    string country = 4;
  }

  repeated Address addresses = 7; // Массив вложенных объектов

  oneof contact { // Только одно из полей может быть установлено
    string phone_number = 8;
    string alternative_email = 9;
  }

  common.Timestamp created_at = 10; // Импортированный тип
}

// Перечисление - набор именованных констант
enum UserStatus {
  UNKNOWN = 0; // Первое значение должно быть 0
  ACTIVE = 1;
  SUSPENDED = 2;
  DELETED = 3;
}

// Другие сообщения для запросов и ответов
message GetUserRequest {
  string user_id = 1;
}

message ListUsersRequest {
  int32 page_size = 1;
  string page_token = 2;
  string filter = 3;
}

message CreateUserRequest {
  User user = 1;
}

message BatchResponse {
  int32 success_count = 1;
  int32 failure_count = 2;
  repeated string error_messages = 3;
}

message ChatMessage {
  string sender = 1;
  string content = 2;
  common.Timestamp sent_at = 3;
}
#+end_src

Кроме базовых типов, Protobuf поддерживает комплексные типы данных и специальные конструкции:
1. =Вложенные типы= — можно определять сообщения и перечисления внутри других сообщений, что помогает организовывать сложные схемы данных.
2. =Repeated= поля — аналог массивов или списков, позволяющие хранить несколько значений одного типа.
3. =Oneof= — специальная конструкция для моделирования взаимоисключающих полей, когда только одно из нескольких полей может быть установлено.
4. =Map= — ассоциативные массивы, появившиеся в Proto3.
5. =Расширения= (Extensions) — в Proto2 позволяют добавлять поля к существующим сообщениям без изменения их определения (в Proto3 заменены типом Any).

Помимо типов данных и генерации кода, Protocol Buffers также предоставляют богатые возможности для валидации и документирования схемы данных.
С помощью комментариев и специальных аннотации в .proto файлах, можно создавать самодокументируемые контракты API:
#+begin_src go
// Пользователь системы
message User {
  // Уникальный идентификатор пользователя
  // Должен соответствовать формату UUID v4
  string id = 1 [(validate.rules).string.pattern = "^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$"];

  // Полное имя пользователя
  string name = 2 [(validate.rules).string.min_len = 2, (validate.rules).string.max_len = 100];

  // Email пользователя для связи
  string email = 3 [(validate.rules).string.email = true];

  // ...
}
#+end_src

Такой подход к документации и валидации, встроенный прямо в схему данных, значительно упрощает поддержку и развитие микросервисной архитектуры, особенно когда над ней работает несколько команд.

При изменении схемы .proto файлов нужно быть уверенным, что эти изменения не нарушат работу существующих клиентов. Хотя Protobuf обеспечивает определёную степень обратной совместимости, есть операции, которые могут её нарушить:
- Удаление полей или изменение их типов.
- Изменение тегов (номеров) полей.
- Переименование полей (хотя сам Protobuf этого не "видит", но сгенерированный код изменится).

Поэтому в продакшн-системах мы обычно следуем следущему подходу:
1. Никогда не удаляем поля — вместо этого помечаем их как устаревшие (deprecated).
2. Никогда не меняем теги полей — даже если поле переименовывается, его тег должен остаться прежним.
3. Контролируем обратную совместимость автоматически с помощью инструментов типа protolock.

* Установка gRPC в Go
Перед началом работы необходимо установить пакет gRPC и компилятор Protocol Buffers:
#+begin_src
go install google.golang.org/protobuf/cmd/protoc@latest
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest
#+end_src

Эти инструменты нужны для генерации Go-кода из .proto файлов. Теперь добавим зависимости в проект:
#+begin_src
go get google.golang.org/grpc
go get google.golang.org/protobuf
#+end_src

* Установка gRPC в С++
#+begin_src
# Установка базовых инструментов
sudo apt-get update
sudo apt-get install -y build-essential cmake autoconf libtool pkg-config

# Клонирование и установка gRPC вместе с Protobuf
git clone --recurse-submodules -b v1.76.0 https://github.com/grpc/grpc
cd grpc
mkdir -p cmake/build
cd cmake/build
cmake -DgRPC_INSTALL=ON -DgRPC_BUILD_TESTS=OFF -DCMAKE_INSTALL_PREFIX=$HOME/.local ../..
make -j$(nproc)
make install
#+end_src

* Определение gRPC-сервиса
В gRPC API описывается с помощью файла =.proto=. Давайте создадим сервис для управления пользователями:
#+begin_src go
syntax = "proto3";

package main;

option go_package = "./pb";

service Greeter {
  rpc SayHello (HelloRequest) returns (HelloReply);
}

message HelloRequest {
  string name = 1;
}

message HelloReply {
  string message = 1;
}
#+end_src

Теперь сгенерируем код для Go:
#+begin_src
protoc --go_out=. --go-grpc_out=. hello.proto
#+end_src

* Реализация сервера
#+begin_src go
package main

import (
	"context"
	"log"
	"net"

	"google.golang.org/grpc"
	pb "grpcex/pb" // Импортируем сгенерированный код
)

type server struct {
	pb.UnimplementedGreeterServer
}

func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {
	return &pb.HelloReply{Message: "Hello " + in.Name}, nil
}

func main() {
	lis, err := net.Listen("tcp", ":50051")
	if err != nil {
		log.Fatalf("Failed to listen: %v", err)
	}

	s := grpc.NewServer()
	pb.RegisterGreeterServer(s, &server{})

	log.Println("Server listening on :50051")
	if err := s.Serve(lis); err != nil {
		log.Fatalf("Failed to serve: %v", err)
	}
}
#+end_src

* Реализация клиента
#+begin_src go
package main

import (
	"context"
	"log"
	"time"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure" // Используем insecure для упрощения
	pb "grpcex/pb" // Импортируем сгенерированный код
)

func main() {
	conn, err := grpc.Dial("localhost:50051", grpc.WithTransportCredentials(insecure.NewCredentials()))
	if err != nil {
		log.Fatalf("Failed to connect: %v", err)
	}
	defer conn.Close()

	client := pb.NewGreeterClient(conn)

	resp, err := client.SayHello(context.Background(), &pb.HelloRequest{Name: "World"})
	if err != nil {
		log.Fatalf("Failed to call SayHello: %v", err)
	}

	log.Printf("Response: %s", resp.Message)
}
#+end_src
